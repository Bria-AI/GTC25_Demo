{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code requires a Bria API token: https://platform.bria.ai/register "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"api_token\": os.getenv(\"BRIA_API_TOKEN\")  # Ensure BRIA_API_TOKEN is set in your environment variables\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from io import BytesIO\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import gc\n",
    "import tempfile\n",
    "\n",
    "def display_pil_image(img: Image.Image, resize: int = None):\n",
    "    \"\"\"\n",
    "    Efficiently displays a PIL Image in a Jupyter Notebook with optional resizing.\n",
    "    \n",
    "    Args:\n",
    "        img (PIL.Image.Image): The image to display.\n",
    "        resize (int, optional): The desired width of the image. \n",
    "                                The height is automatically adjusted to maintain aspect ratio.\n",
    "                                If None, the original size is used.\n",
    "    \"\"\"\n",
    "    if not isinstance(img, Image.Image):\n",
    "        raise ValueError(\"Input must be a PIL Image object\")\n",
    "\n",
    "    # Resize image while maintaining aspect ratio\n",
    "    if resize:\n",
    "        width, height = img.size\n",
    "        aspect_ratio = height / width  # Maintain original aspect ratio\n",
    "        new_size = (resize, int(resize * aspect_ratio))\n",
    "        img.thumbnail(new_size, Image.LANCZOS)  # Resize in place\n",
    "\n",
    "    # Save the processed image to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmpfile:\n",
    "        img.save(tmpfile.name)\n",
    "        temp_filename = tmpfile.name  # Store file path\n",
    "\n",
    "    # Display using IPython.display.Image\n",
    "    display(IPyImage(filename=temp_filename))\n",
    "\n",
    "    # Explicitly delete the image object and force garbage collection\n",
    "    del img\n",
    "    gc.collect()\n",
    "\n",
    "def add_title_to_image(image, text, font_path='ArialBold.ttf', font_size=20, text_color=\"black\", padding=10):\n",
    "\n",
    "    # Load the font\n",
    "    if font_path:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    else:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Calculate the text size and position\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "    image_width, image_height = image.size\n",
    "    title_height = text_height + 2 * padding\n",
    "    new_image_height = image_height + title_height\n",
    "    \n",
    "    # Create a new image with space for the title\n",
    "    new_image = Image.new('RGB', (image_width, new_image_height), \"white\")\n",
    "    new_image.paste(image, (0, title_height))\n",
    "    \n",
    "    # Draw the title on the new image\n",
    "    draw = ImageDraw.Draw(new_image)\n",
    "    text_position = ((image_width - text_width) // 2, padding)\n",
    "    draw.text(text_position, text, font=font, fill=text_color)\n",
    "    \n",
    "    return new_image    \n",
    "\n",
    "def display_images(image_list, title = \"\", line_width=10, resize = 350, font_size=15, return_img=False):\n",
    "    # resize images while keeping aspect ratio\n",
    "    resized_images = []\n",
    "    for img in image_list:\n",
    "        img.thumbnail((resize, resize))\n",
    "        resized_images.append(img)\n",
    "\n",
    "    image_list = resized_images\n",
    "    # Calculate the total width and the maximum height of the concatenated image\n",
    "    total_width = sum(image.width for image in image_list) + line_width * (len(image_list) - 1)\n",
    "    max_height = max(image.height for image in image_list)\n",
    "    \n",
    "    # Create a new blank image with the calculated dimensions and fill it with white\n",
    "    concatenated_image = Image.new('RGB', (total_width, max_height), 'black')\n",
    "    \n",
    "    # Paste each image into the new image\n",
    "    x_offset = 0\n",
    "    for image in image_list:\n",
    "        concatenated_image.paste(image, (x_offset, 0))\n",
    "        x_offset += image.width + line_width\n",
    "    \n",
    "    if title:\n",
    "        concatenated_image = add_title_to_image(concatenated_image, title, font_size=font_size)\n",
    "    # return concatenated_image\n",
    "    display_pil_image(concatenated_image)\n",
    "\n",
    "    if return_img:\n",
    "        return concatenated_image\n",
    "\n",
    "def return_images_from_urls(image_urls):\n",
    "    images = []\n",
    "    for image_url in image_urls:\n",
    "        img_response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(img_response.content))\n",
    "        images.append(img)\n",
    "    return images    \n",
    "\n",
    "def pad_image_to_square(img):\n",
    "    width, height = img.size\n",
    "    max_dim = max(width, height)\n",
    "    new_img = Image.new('RGB', (max_dim, max_dim), (0, 0, 0))\n",
    "    new_img.paste(img, ((max_dim - width) // 2, (max_dim - height) // 2))\n",
    "    return new_img\n",
    "\n",
    "# Function to convert a PIL Image to a base64-encoded string\n",
    "def pil_image_to_base64(pill_image):\n",
    "    buffered = BytesIO()\n",
    "    pill_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return encoded_string    \n",
    "\n",
    "# show mask overlayed on the image\n",
    "def display_mask(mask, image):\n",
    "    image = image.copy()\n",
    "    mask = mask.resize(image.size)\n",
    "    mask = mask.convert(\"RGBA\")\n",
    "    mask.putalpha(128)\n",
    "    image.paste(mask, (0, 0), mask)\n",
    "    display_images([image], f\"Gen-Fill Mask Overlay\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Bria: AI-Powered Visual Content Generation\n",
    "\n",
    "Bria is... ???an advanced AI-driven tool for builders to generating and modifying brand-consistent visuals at scale. ???\n",
    "\n",
    "This demo will showcase how developers can leverage Bria's APIs to build tools that enable their users to generate and modify brand-consistent visuals at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/bria_intro1.jpg\")], resize=600)\n",
    "display_images([Image.open(\"./visuals/bria_intro2.jpg\")], resize=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Gen-AI for Brand Content Creation\n",
    "\n",
    "We'll use **Bria's API** to show how a developer can easily **build** tools for creating and editing **controlled**, **on-brand** visuals **at scale**.\n",
    "\n",
    "A strong brand identity includes visual features such as:\n",
    "\n",
    "- **Color Palette**\n",
    "- **Style & Mood**\n",
    "- **Recurring Characters (\"mascot\") & Themes** \n",
    "- **Fonts** \n",
    "- **Logo**\n",
    "\n",
    "The example below showcases how such features are present in the Bria brand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/bria_brand_example.png\")], resize=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text-to-Image**: AI-Generated Visuals\n",
    "\n",
    "Visual GenAI starts with text-to-image generation. \n",
    "\n",
    "Below, we define a function to generate images from a prompt using Bria's **text-to-image API**:\n",
    "\n",
    "https://docs.bria.ai/image-generation/endpoints/text-to-image-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image(prompt, num_results=3, model_version=\"2.3\", fast=True, seed=42):\n",
    "    \"\"\"\n",
    "    Generate AI-powered images from text prompts using Bria's API.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): The text description of the desired image.\n",
    "    - model_version (str): foundation model version to use.\n",
    "    - num_results (int): Number of images to generate.\n",
    "    - fast (bool): Whether to use the fast generation endpoint.\n",
    "    - seed (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - List of generated image objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = f\"https://engine.prod.bria-api.com/v1/text-to-image/base/{model_version}\"\n",
    "    if fast:\n",
    "        base_url = f\"https://engine.prod.bria-api.com/v1/text-to-image/fast/{model_version}\"\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt, \n",
    "        \"seed\": seed, \n",
    "        \"num_results\": num_results,\n",
    "        \"sync\": True, \n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result\", [{}])\n",
    "    print(responses[0].get(\"warning\",\"\"))\n",
    "    image_urls = [x.get(\"urls\", [None])[0] for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Generating an Image with Bria\n",
    "# The following example demonstrates how Bria's API generates AI-driven visuals from text descriptions.\n",
    "\n",
    "prompt = 'A 3D render of an elephant with purple skin, over white background'\n",
    "\n",
    "images = text_to_image(prompt, num_results=3)\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bria’s Responsible AI**\n",
    "\n",
    "Bria ensures content safety by preventing the generation of copyrighted material. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A 3D render of an elephant with purple skin, that resembels Disney's Dumbo\"\n",
    "images = text_to_image(prompt)\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating the same prompt with Flux we get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals//flux_dumbo.png\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing The Brand Mascot\n",
    "\n",
    "We want to enable users to accurately create visuals using their brand assets, such as the following examples of the **Bria Elephant**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample of bria elephant oiriginal images\n",
    "\n",
    "bria_bear_dir = \"briaphant\"\n",
    "images = [Image.open(f\"{bria_bear_dir}/{f}\") for f in os.listdir(bria_bear_dir)][:4]\n",
    "display_images(images, \"Bria Elephant - Originals\", resize = 1000, font_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add more components to the text-to-image generation to increase the controlability. We'll start by introducing **ControlNets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Guidance for Controlled Generation\n",
    "We can add **structural control** using an input image to generate variations of that image, using the following Control Nets which were trained on-top of Bria's foundation text-to-image model:\n",
    "- Canny\n",
    "- Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/control_nets.png\")], resize=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose one of the Bria Elephant original images and add structural image guidance to the generation using our trained ControlNets.\n",
    "\n",
    "We can use Bria's **Reimagine API** which combines both ControlNets to reproduce the structure of the input image, while allowing changes through the textual prompt:\n",
    "\n",
    "https://docs.bria.ai/image-generation/endpoints/reimagine-structure-reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image_with_guidance(prompt, guidance_image, num_results=3, fast=True, seed=42):\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/reimagine\"\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt, \n",
    "        \"seed\": seed, \n",
    "        \"num_results\": num_results,\n",
    "        \"sync\": True, \n",
    "        \"structure_image_file\": pil_image_to_base64(guidance_image),\n",
    "        \"fast\": fast\n",
    "    }\n",
    "    \n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result\", [{}])\n",
    "    image_urls = [x.get(\"urls\", [None])[0] for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:\n",
    "prompt = 'A 3D render of an elephant with purple skin, over white background'\n",
    "guidance_image = Image.open(\"./briaphant/bria_1afcb261_2000_49eb_a908_3656fd9a67fd_4.png\")\n",
    "\n",
    "images = text_to_image_with_guidance(prompt, guidance_image, num_results=2)\n",
    "display_images([guidance_image], 'Input Image')\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Reimagine to change the color of the Bria Elephant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for color in ['blue', 'green', 'brown', 'rainbow colored']:\n",
    "    prompt = f'A 3D render of an elephant with {color} skin, over white background'\n",
    "    \n",
    "    guidance_image = Image.open(\"./briaphant/bria_1afcb261_2000_49eb_a908_3656fd9a67fd_4.png\")\n",
    "\n",
    "    image = text_to_image_with_guidance(prompt, guidance_image, num_results=1)\n",
    "    images.append(image[0])\n",
    "display_images(images, 'prompt: A 3D render of an elephant with {color} skin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases this structural control is not enough. We want to enable our users to teach the model to generate a more accurate and varied represenation of this character.\n",
    "\n",
    "We will allow this by enabling our users to **fine-tune** our foundation model using the original brand images they own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailored-Generation\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the images we have of the Bria Elephant to fine-tune Bria's foundation model to be able to generate this chararcter.\n",
    "\n",
    "\n",
    "To train a model we need some data. 5 images of the same character could be enough, but generally the more the better.\n",
    "The data consists of images and captions. We can use a VLM as a captioner. Let's use Amazon Bedrock to prompt a vision-language model like Claude Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready, we can fine-tune the model using LoRA...\n",
    "show how to download model weights from HF and run train_lora code on local machine with X VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to another notebook for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Bria API you can do all that (including automatic captioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We'll use Bria's Generate Image - Tailored Model API:\n",
    "\n",
    "https://docs.bria.ai/tailored-generation/endpoints/text-to-image-tailored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tailored_gen(prompt, tailored_model, num_results=3, seed=42):\n",
    "    \n",
    "    base_url = f\"https://engine.prod.bria-api.com/v1/text-to-image/tailored/{tailored_model}\"\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt, \n",
    "        \"seed\": seed, \n",
    "        \"num_results\": num_results,\n",
    "        \"sync\": True, \n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result\", [{}])\n",
    "    image_urls = [x.get(\"urls\", [None])[0] for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"holding a pink cocktail in its trunk and wearing a small pink party hat\"\n",
    "tailored_model = 10920\n",
    "elephant_image = tailored_gen(prompt, tailored_model, num_results=1)[0]\n",
    "# elephant_image.save('elephant_image.jpg')\n",
    "display_images([elephant_image], f\"prompt: \\n{prompt}\", font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's place this festive character in a proper location. \n",
    "\n",
    "We want the background to adhere to the brand style as well, so we'll use a tailored model trained on the following brand style images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample of bria style oiriginal images\n",
    "bria_style_dir = \"bria_style\"\n",
    "\n",
    "images = [Image.open(f\"{bria_style_dir}/{f}\") for f in os.listdir(bria_style_dir)][:4]\n",
    "display_images(images, \"Bria Style - Originals\", resize = 1000, font_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an LLM to write a few prompts for background images that could be relevant for this character in a festive event. \n",
    "\n",
    "Let's generate 1 example from each, using the brand style tailored model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_prompts = [\n",
    " 'A tropical beach at sunset with palm trees, soft waves, and string lights, perfect for a relaxing party vibe',\n",
    " 'A winter wonderland party with twinkling snowflakes, icy-blue lighting, and festive holiday purple decorations creating a cozy atmosphere',\n",
    " 'A cosmic galaxy party with glowing planets, swirling nebulas, and a dance floor that looks like the surface of the moon.',\n",
    " 'A lively party venue with colorful decorations, balloons, streamers, and warm lighting, creating a fun and festive atmosphere',\n",
    " 'A carnival-themed party with bright lights, a Ferris wheel in the background, colorful booths, and festive decorations',\n",
    " 'A futuristic space party with floating balloons, glowing neon decorations, and a starry galaxy sky in the background',\n",
    " \"A retro '80s party with neon colors, arcade machines, a checkered dance floor, and a boombox playing classic hits.\",\n",
    " 'A jungle adventure party with tropical foliage, tiki torches, tribal drums, and exotic animals hidden in the background.',\n",
    " 'A rooftop sunset cocktail party with stylish lounge seating, golden hour lighting, and a panoramic city skyline view.'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bg_images = []\n",
    "# for i, prompt in enumerate(background_prompts):\n",
    "#     tailored_model = 10900\n",
    "#     bg_image = tailored_gen(prompt, tailored_model, num_results=1)[0]\n",
    "#     bg_image.save(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_originals/{i}.png')\n",
    "#     all_bg_images.append(bg_image)\n",
    "\n",
    "all_bg_images = [Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_originals/{i}.png') for i in range(len(background_prompts))]\n",
    "display_images(all_bg_images[:3], f\"Generated Backgrounds from Bria Style Tailored Model\", resize = 256)\n",
    "display_images(all_bg_images[3:6], resize = 256)\n",
    "display_images(all_bg_images[6:9], resize = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Avoid text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Genreation (by Reference Image)\n",
    "Next, we want to use those brand style backgrounds as inspiration for new background for our festive Bria Elephant. \n",
    "\n",
    "We'll use Bria's Generate Background API:\n",
    "\n",
    "\n",
    "https://docs.bria.ai/image-editing/endpoints/background-replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bg_by_image(foreground_image, bg_image, num_results=1, seed=42, padding=[0, 0, 0, 0]):\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/background/replace\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(foreground_image),\n",
    "        \"ref_image_file\": pil_image_to_base64(bg_image),\n",
    "        \"sync\": True,\n",
    "        \"placement_type\": \"manual_padding\",\n",
    "        \"num_results\": num_results,\n",
    "        \"seed\": seed,         \n",
    "        \"padding_values\": padding\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result\", [{}])\n",
    "    image_urls = [x[0] for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_image = Image.open('elephant_image.jpg')\n",
    "all_bg_images = [Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_originals/{i}.png') for i in range(len(background_prompts))]\n",
    "resize_scale = 0.25 # we'll make the elephant 25% smaller to fit into the background images\n",
    "\n",
    "# we want the elephant to be positioned in the bottom right corner of the background image, so we'll add padding accordingly\n",
    "width_padding = int(foreground_image.size[0]*resize_scale)\n",
    "height_padding = int(foreground_image.size[1]*resize_scale)\n",
    "bottom_right_location = [width_padding, 0, height_padding, 0]\n",
    "\n",
    "all_images = []\n",
    "for i, bg_img in enumerate(all_bg_images):\n",
    "\n",
    "    new_bg_img = generate_bg_by_image(foreground_image, bg_img, num_results=1, padding=bottom_right_location)\n",
    "    new_bg_img[0].save(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_generations/{i}.png')\n",
    "    all_images.append(new_bg_img[0])\n",
    "\n",
    "# all_images = [Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_generations/{i}.png') for i in range(len(background_prompts))]\n",
    "\n",
    "display_images(all_images[:3], f\"Replaced Backgrounds with Reference Images\", resize = 256)\n",
    "display_images(all_images[3:6], resize = 256)\n",
    "display_images(all_images[6:9], resize = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Editing: Generative Fill\n",
    "\n",
    "Focusing on the first 3 outputs, let's fix some content issues by replacing or adding content using Bria's Generative-Fill API:\n",
    "\n",
    "https://docs.bria.ai/image-editing/endpoints/gen-fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fill(original_image, mask, prompt, num_results=1, seed=42):\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/gen_fill\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(original_image),\n",
    "        \"mask_file\": pil_image_to_base64(mask.convert('RGB')),\n",
    "        \"mask_type\": \"manual\",\n",
    "        \"prompt\": prompt,\n",
    "        \"num_results\": num_results,\n",
    "        \"sync\": True,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"urls\", [])\n",
    "    image_urls = [x for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is a beach scene, let's replace the party hat with some beachwear. We'll use a mask around the hat we want to replace, and specifiy the new content in \"prompt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_generations/0.png')\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/hat_mask.png')\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var2 = erase(input_image, mask)[0]\n",
    "display_images([output_var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = output_var2\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/hat_mask_2.png')\n",
    "prompt = \"a straw hat\"\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var1 = gen_fill(input_image, mask, prompt)[0]\n",
    "display_images([output_var1], f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace the hat in the second image to something more suitable for winter festivities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_generations/1.png')\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/hat_mask.png')\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var2 = erase(input_image, mask)[0]\n",
    "display_images([output_var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = output_var2\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/hat_mask.png')\n",
    "prompt = \"a santa hat\"\n",
    "\n",
    "output_var2 = gen_fill(input_image, mask, prompt)[0]\n",
    "display_images([output_var2], f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a christmas tree in the brand colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = output_var2\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/tree_mask.png')\n",
    "prompt = \"a lush christmas tree with purple and pink ornaments\"\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var2 = gen_fill(input_image, mask, prompt)[0]\n",
    "display_images([output_var2], f\"prompt: \\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Editing: Eraser\n",
    "And finally, the summer cocktail doesn't align with the winter theme, so let's remove it. \n",
    "\n",
    "We'll use Bria's Eraser API:\n",
    "\n",
    "https://docs.bria.ai/image-editing/endpoints/eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase(original_image, mask):\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/eraser\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(original_image),\n",
    "        \"mask_file\": pil_image_to_base64(mask.convert('RGB')),\n",
    "        \"mask_type\": \"manual\",\n",
    "        \"sync\": True,\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result_url\", '')\n",
    "    image_urls = [responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = output_var2\n",
    "mask = Image.open('/home/ubuntu/spring/demo_gtc/masks/cocktail_mask.png')\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var2 = erase(input_image, mask)[0]\n",
    "display_images([output_var2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Editing: Expand\n",
    "\n",
    "We now have our 3 image varaiations ready. But what if we wanted to use them in adds with different aspect ratios? \n",
    "\n",
    "We'll use Bria's Expand Image API to expand the image to different aspect ratios:\n",
    "\n",
    "https://docs.bria.ai/image-editing/endpoints/image-expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_var3 = Image.open(f'/home/ubuntu/spring/demo_gtc/bg_gen/bg_generations/2.png').resize((350, 350))\n",
    "image_variations = [output_var1, output_var2, output_var3]\n",
    "display_images(image_variations, resize = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_left(input_image, seed=1000):\n",
    "    org_width, org_height = input_image.size\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/image_expansion\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(input_image),\n",
    "        \"canvas_size\": [\n",
    "            org_width*2,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_size\": [\n",
    "            org_width,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_location\": [\n",
    "            org_width,\n",
    "            0\n",
    "        ],\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result_url\", '')\n",
    "    image_urls = [responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n",
    "\n",
    "def expand_top(input_image, seed=1000):\n",
    "    org_width, org_height = input_image.size\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/image_expansion\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(input_image),\n",
    "        \"canvas_size\": [\n",
    "            org_width,\n",
    "            org_height*2\n",
    "        ],\n",
    "        \"original_image_size\": [\n",
    "            org_width,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_location\": [\n",
    "            0,\n",
    "            org_height\n",
    "        ],\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result_url\", '')\n",
    "    image_urls = [responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_images_left = []\n",
    "for image_var in image_variations:\n",
    "    expanded_images_left.append(expand_left(image_var)[0])\n",
    "\n",
    "display_images(expanded_images_left, resize = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_images_top = []\n",
    "for image_var in image_variations:\n",
    "    expanded_images_top.append(expand_top(image_var)[0])\n",
    "\n",
    "display_images(expanded_images_top, resize = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(image_variations)\n",
    "display_images([pad_image_to_square(x) for x in expanded_images_top])\n",
    "display_images([pad_image_to_square(x) for x in expanded_images_left])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### increase res!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

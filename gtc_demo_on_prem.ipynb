{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This notebook was tested on Amazon EC2 G6e Instance (NVIDIA L40S Tensor Core GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code requires a Hugging Face token with Bria access\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"your-huggingface-token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Bria: AI-Powered Visual Content Generation\n",
    "\n",
    "Bria is a visul-Gen-AI platform for builders, designed for commercial use and powered exclusively by licensed data.\n",
    "\n",
    "Bria offers API & source-code models for safe and predictable visual Gen-AI development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/bria_intro1.jpg\"), Image.open(\"./visuals/bria_intro2.jpg\")], resize=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo will showcase how developers can leverage Bria's **source-available** models **on-prem** to build tools that enable their users to generate and modify brand-consistent visuals at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Gen-AI for Brand Content Creation\n",
    "\n",
    "We'll use **Bria's models**, accesible through **Hugging Face**, to show how a developer can easily **build** tools for creating and editing **controlled**, **on-brand** visuals **at scale**.\n",
    "\n",
    "A strong brand identity includes visual features such as:\n",
    "\n",
    "- **Color Palette**\n",
    "- **Style & Mood**\n",
    "- **Recurring Characters (\"mascot\") & Themes** \n",
    "- **Fonts** \n",
    "- **Logo**\n",
    "\n",
    "The example below showcases how such features are present in the Bria brand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/bria_brand_example.png\")], resize=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text-to-Image**: AI-Generated Visuals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Visual GenAI starts with text-to-image generation. \n",
    "\n",
    "Below, we define a function to generate images from a prompt using Bria-2.3 foundation text-to-image model by using the model weights available on HF: https://huggingface.co/briaai/BRIA-2.3\n",
    "\n",
    "We'll start with a basic Text-to-Image (t2i) diffusers pipeline.\n",
    "\n",
    "(Also available through API: https://docs.bria.ai/image-generation/endpoints/text-to-image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "t2i_pipe = DiffusionPipeline.from_pretrained(\"briaai/BRIA-2.3\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "t2i_pipe.force_zeros_for_empty_prompt = False\n",
    "t2i_pipe.to(\"cuda\")\n",
    "\n",
    "def text_to_image(prompt, num_results=3, seed=42):\n",
    "    negative_prompt = \"Logo,Watermark,Text,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers\"\n",
    "\n",
    "    images = []\n",
    "    for i in range(num_results):\n",
    "        generator = torch.Generator(device=\"cuda:0\").manual_seed(seed+i) \n",
    "        image = t2i_pipe(prompt=prompt, negative_prompt=negative_prompt, height=1024, width=1024, generator=generator, num_inference_steps=30).images[0]\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Generating an Image with Bria\n",
    "# The following example demonstrates how Bria's API generates AI-driven visuals from text descriptions.\n",
    "\n",
    "prompt = 'A 3D render of a purple skinned elephant, over white background'\n",
    "\n",
    "images = text_to_image(prompt, num_results=3)\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briaâ€™s Responsible AI**\n",
    "\n",
    "Bria ensures content safety by preventing the generation of copyrighted material. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A 3D render of a purple skinned elephant, that resembels Disney's Dumbo\"\n",
    "images = text_to_image(prompt)\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating the same prompt with Flux we get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals//flux_dumbo.png\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing The Brand Mascot\n",
    "\n",
    "We want to enable users to accurately create visuals using their brand assets, such as the following examples of the **Bria Elephant**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample of bria elephant oiriginal images\n",
    "\n",
    "bria_bear_dir = \"briaphant\"\n",
    "images = [Image.open(f\"{bria_bear_dir}/{f}\") for f in os.listdir(bria_bear_dir) if \"png\" in f][:4]\n",
    "display_images(images, \"Bria Elephant - Originals\", resize = 1000, font_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add more components to the text-to-image generation to increase the controlability. We'll start by introducing **Control-Nets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Guidance for Controlled Generation\n",
    "We can add **structural control** using an input image to generate variations of that image, using the following Control Nets which were trained on-top of Bria's foundation text-to-image model:\n",
    "- Canny\n",
    "- Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/control_nets.png\")], resize=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose one of the Bria Elephant original images and add structural image guidance to the generation using our trained Control-Nets:\n",
    "\n",
    "https://huggingface.co/briaai/BRIA-2.3-ControlNet-Canny\n",
    "\n",
    "https://huggingface.co/briaai/BRIA-2.3-ControlNet-Depth\n",
    "\n",
    "We'll create a second diffusers pipeline that will integrate the two Control-Nets to reproduce the structure of the input image, while allowing changes through the textual prompt.\n",
    "\n",
    "(Also available through the text-to-image API as well as the \"Reimagine\" API: \n",
    "https://docs.bria.ai/image-generation/endpoints/reimagine-structure-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"briaai/BRIA-2.3-ControlNet-Canny\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    \"briaai/BRIA-2.3-ControlNet-Depth\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "t2i_control_pipe = StableDiffusionXLControlNetPipeline.from_pipe(\n",
    "                    t2i_pipe,\n",
    "                    controlnet=[controlnet_canny, controlnet_depth],\n",
    "                )\n",
    "t2i_control_pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process image for the canny required input\n",
    "def get_canny_image(img_path):\n",
    "    # Calculate Canny image\n",
    "    input_image = load_image(\n",
    "        img_path\n",
    "    )\n",
    "    input_image = np.array(input_image)\n",
    "    low_threshold, high_threshold = 100, 200\n",
    "    input_image = cv2.Canny(input_image, low_threshold, high_threshold)\n",
    "    input_image = input_image[:, :, None]\n",
    "    input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
    "    canny_image = Image.fromarray(input_image)\n",
    "    return canny_image\n",
    "\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from torchvision import transforms\n",
    "\n",
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "# Function to process image for the depth required input (depth map)\n",
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "    image = transforms.functional.center_crop(image, min(image.shape[-2:]))\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "\n",
    "def text_to_image_with_guidance(prompt, guidance_image_path, num_results=3, seed=42):\n",
    "    negative_prompt = \"blurry\"\n",
    "    canny_img = get_canny_image(guidance_image_path)\n",
    "    depth_img = get_depth_map(Image.open(guidance_image_path))\n",
    "\n",
    "    images = []\n",
    "    for i in range(num_results):\n",
    "        generator = torch.Generator(device=\"cuda:0\").manual_seed(seed+i)\n",
    "        image = t2i_control_pipe(prompt=prompt, negative_prompt=negative_prompt, image=[canny_img, depth_img], controlnet_conditioning_scale=[0.5, 0.5], height=1024, width=1024, generator=generator, num_inference_steps=30).images[0]\n",
    "        images.append(image)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A 3D render of a purple skinned elephant, over white background'\n",
    "guidance_image_path = \"./briaphant/bria_1afcb261_2000_49eb_a908_3656fd9a67fd_4.png\"\n",
    "\n",
    "images = text_to_image_with_guidance(prompt, guidance_image_path, num_results=2)\n",
    "display_images([Image.open(guidance_image_path)], 'Input Image')\n",
    "display_images(images, f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Control-Nets to change the color of the Bria Elephant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for color in ['blue', 'green', 'brown', 'rainbow colored']:\n",
    "    prompt = f'A 3D render of a {color} skinned elephant, over white background'\n",
    "\n",
    "    image = text_to_image_with_guidance(prompt, guidance_image_path, num_results=1)\n",
    "    images.append(image[0])\n",
    "display_images(images, 'prompt: A 3D render of a {color} skinned elephant, over white background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases this structural control is not enough. We want to enable our users to teach the model to generate a more accurate and varied represenation of this character.\n",
    "\n",
    "We will allow this by enabling our users to **fine-tune** our foundation model using the original brand images they own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailored-Generation - Fine-Tuning with LoRA\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune Bria's foundation model using the existing images of the Bria Elephant. For fine-tuning we would use Bria's **4B-Adapt** model, which is designed to provide exceptional fine-tuning capabilities for commercial use: https://huggingface.co/briaai/BRIA-4B-Adapt\n",
    "\n",
    "\n",
    "We fine-tune using LoRA for easier deployment of each such fine-tuned model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please refer to the following examples:\n",
    "\n",
    "- Training using Bria's automatic Tailored-Gen API: gtc_demo_fine_tune_api.ipynb\n",
    "\n",
    "- Training on-prem using Bria's foundation model weights on HF: gtc_demo_fine_tune_on_prem.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Once we finished training and we have the LoRA trained weights available, we can run inference using the HF available weights:\n",
    "\n",
    "https://huggingface.co/briaai/BRIA-4B-Adapt\n",
    "\n",
    "\n",
    "(Also available through Bria's API: https://docs.bria.ai/tailored-generation/endpoints/text-to-image-tailored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download neccesary python files from HF model card\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "try:\n",
    "    local_dir = os.path.dirname(__file__)\n",
    "except:\n",
    "    local_dir = '.'\n",
    "    \n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='pipeline_bria.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='transformer_bria.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='bria_utils.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='train_lora.py', local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pipeline_bria import BriaPipeline\n",
    "\n",
    "tailored_pipe = BriaPipeline.from_pretrained(\"briaai/BRIA-4B-Adapt\", torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "tailored_pipe.to(device=\"cuda\")\n",
    "\n",
    "\n",
    "def tailored_gen(prompt, tailored_model_name, seed=42, lora_scale=1.0):\n",
    "    \n",
    "    negative_prompt = \"Logo,Watermark,Text,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers\"\n",
    "\n",
    "    tailored_pipe.load_lora_weights(\"briaai/BRIA-4B-Adapt\", subfolder=\"example_finetuned_model\", weight_name = f\"{tailored_model_name}.safetensors\")\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    image = tailored_pipe(prompt=prompt, negative_prompt=negative_prompt, height=1024, width=1024, generator=generator, joint_attention_kwargs={\"scale\": lora_scale}, num_inference_steps=30).images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tuned using image captions that share the same prefix, which we'll use for the inference prompt as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photo of a character named Briaphant, a purple elephant, holding a pink cocktail in its trunk and wearing a small pink party hat\"\n",
    "tailored_model_name = \"bria_elephant\"\n",
    "\n",
    "elephant_image = tailored_gen(prompt, tailored_model_name)\n",
    "elephant_image.save('./on_prem_results/elephant_image.jpg')\n",
    "display_images([elephant_image], f\"prompt: \\n{prompt}\", font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's place this festive character in a proper location by generating on-brand backgrounds.\n",
    "\n",
    "We want the background to adhere to the brand style as well, so we'll use a tailored model trained on the following brand style images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample of bria style oiriginal images\n",
    "bria_style_dir = \"bria_style\"\n",
    "\n",
    "images = [Image.open(f\"{bria_style_dir}/{f}\") for f in os.listdir(bria_style_dir)][:4]\n",
    "display_images(images, \"Bria Style - Originals\", resize = 1000, font_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an LLM to write a few prompts for background images that could be relevant for this character in a festive event. \n",
    "\n",
    "Let's generate 1 example from each, using the brand style tailored model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_prompts = [\n",
    " 'A tropical beach at sunset with palm trees, soft waves, and string lights, perfect for a relaxing party vibe',\n",
    " 'A winter wonderland party with twinkling snowflakes, icy-blue lighting, and festive holiday purple decorations creating a cozy atmosphere',\n",
    " 'A cosmic galaxy party with glowing planets, swirling nebulas, and a dance floor that looks like the surface of the moon.',\n",
    " 'A lively party venue with colorful decorations, balloons, streamers, and warm lighting, creating a fun and festive atmosphere',\n",
    " 'A carnival-themed party with bright lights, a Ferris wheel in the background, colorful booths, and festive decorations',\n",
    " 'A futuristic space party with floating balloons, glowing neon decorations, and a starry galaxy sky in the background',\n",
    " \"A retro '80s party with neon colors, arcade machines, a checkered dance floor, and a boombox playing classic hits.\",\n",
    " 'A jungle adventure party with tropical foliage, tiki torches, tribal drums, and exotic animals hidden in the background.',\n",
    " 'A rooftop sunset cocktail party with stylish lounge seating, golden hour lighting, and a panoramic city skyline view.'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_originals_dir = \"./on_prem_results/bg_gen/bg_originals\"\n",
    "os.makedirs(bg_originals_dir, exist_ok=True)\n",
    "\n",
    "all_bg_images = []\n",
    "for i, prompt in enumerate(background_prompts):\n",
    "    tailored_model_name = \"bria_style\"\n",
    "    bg_image = tailored_gen(prompt, tailored_model_name)\n",
    "    bg_image.save(f'{bg_originals_dir}/{i}.png')\n",
    "    all_bg_images.append(bg_image)\n",
    "\n",
    "all_bg_images = [Image.open(f'{bg_originals_dir}/{i}.png') for i in range(len(background_prompts))]\n",
    "display_images(all_bg_images[:3], f\"Generated Backgrounds from Bria Style Tailored Model\", resize = 256)\n",
    "display_images(all_bg_images[3:6], resize = 256)\n",
    "display_images(all_bg_images[6:9], resize = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Genreation (by Reference Image)\n",
    "\n",
    "\n",
    "\n",
    "Next, we want to use those on-brand backgrounds we created as inspiration for new background that will include our festive Bria Elephant. \n",
    "\n",
    "We'll use a special Control-Net trained to generate backgrounds around a given foreground (in this case: the elephant image): https://huggingface.co/briaai/BRIA-2.3-ControlNet-BG-Gen\n",
    "\n",
    "To use the generated backgrounds as inspiration, we'll add Image-Prompt Adapter (IP-Adapter) to the pipeline, which will enable prompting with a given image instead of a textual description: https://huggingface.co/briaai/Image-Prompt\n",
    "\n",
    "\n",
    "\n",
    "(Also available throught Bria's API: https://docs.bria.ai/image-editing/endpoints/background-replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download neccesary python files from HF model card\n",
    "! huggingface-cli download briaai/BRIA-2.3-ControlNet-BG-Gen --include replace_bg/* --local-dir . --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cuda cache\n",
    "del tailored_pipe\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torch\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "from replace_bg.model.pipeline_controlnet_sd_xl import StableDiffusionXLControlNetPipeline\n",
    "from replace_bg.model.controlnet import ControlNetModel\n",
    "from replace_bg.utilities import resize_image, remove_bg_from_image, paste_fg_over_image, get_control_image_tensor\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"briaai/BRIA-2.3-ControlNet-BG-Gen\", torch_dtype=torch.float16) \n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "scheduler = EulerAncestralDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    "    steps_offset=1\n",
    ")\n",
    "replace_bg_pipe = StableDiffusionXLControlNetPipeline.from_pipe(\n",
    "                    t2i_pipe,\n",
    "                    vae=vae,\n",
    "                    controlnet=controlnet,\n",
    "                    scheduler=scheduler,\n",
    "                )\n",
    "replace_bg_pipe.load_ip_adapter(\"briaai/Image-Prompt\", subfolder='models', weight_name=\"ip_adapter_bria.bin\")\n",
    "\n",
    "replace_bg_pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(image, padding_values):\n",
    "    orig_image = image\n",
    "    old_size = orig_image.size\n",
    "\n",
    "    padding = padding_values\n",
    "    new_size = (old_size[0]+padding[0]+padding[1], old_size[1]+padding[2]+padding[3])\n",
    "    paste_loc = (padding[0], padding[2])\n",
    "    new_image = Image.new(\"RGB\", new_size, (255,255,255))\n",
    "    new_image.paste(orig_image, paste_loc)\n",
    "    return new_image\n",
    "\n",
    "def generate_bg_by_image(foreground_image_path, bg_image, seed=42):\n",
    "\n",
    "    image = load_image(foreground_image_path)\n",
    "    image = resize_image(image)\n",
    "    mask = remove_bg_from_image(foreground_image_path)\n",
    "    control_tensor = get_control_image_tensor(replace_bg_pipe.vae, image, mask)\n",
    "\n",
    "    negative_prompt = \"Logo,Watermark,Text,Ugly,Bad proportions,Bad quality,Out of frame,Mutation\"\n",
    "    generator = torch.Generator(device=\"cuda:0\").manual_seed(seed)\n",
    "\n",
    "    gen_img = replace_bg_pipe(\n",
    "        ip_adapter_image=bg_image.resize((224, 224)),\n",
    "        negative_prompt=negative_prompt, \n",
    "        prompt=\"high quality\",     \n",
    "        controlnet_conditioning_scale=1.0, \n",
    "        num_inference_steps=30,\n",
    "        image = control_tensor,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    result_image = paste_fg_over_image(gen_img, image, mask)\n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_generations_dir = \"./on_prem_results/bg_gen/bg_generations\"\n",
    "os.makedirs(bg_generations_dir, exist_ok=True)\n",
    "\n",
    "all_bg_images = [Image.open(f'{bg_originals_dir}/{i}.png') for i in range(len(background_prompts))]\n",
    "\n",
    "foreground_image = Image.open('./on_prem_results/elephant_image.jpg')\n",
    "\n",
    "resize_scale = 0.25 # we'll make the elephant 25% smaller to fit into the background images\n",
    "# we want the elephant to be positioned in the bottom right corner of the background image, so we'll add padding accordingly\n",
    "width_padding = int(foreground_image.size[0]*resize_scale)\n",
    "height_padding = int(foreground_image.size[1]*resize_scale)\n",
    "bottom_right_location = [width_padding, 0, height_padding, 0]\n",
    "\n",
    "foreground_image_padded = pad_image(foreground_image, bottom_right_location)\n",
    "foreground_image_path = './on_prem_results/elephant_image_padded.jpg'\n",
    "foreground_image_padded.save(foreground_image_path)\n",
    "\n",
    "all_images = []\n",
    "for i, bg_img in enumerate(all_bg_images):\n",
    "    \n",
    "    new_bg_img = generate_bg_by_image(foreground_image_path, bg_img)\n",
    "    new_bg_img.save(f'{bg_generations_dir}/{i}.png')\n",
    "    all_images.append(new_bg_img)\n",
    "\n",
    "# all_images = [Image.open(f'{bg_generations_dir}/{i}.png') for i in range(len(background_prompts))]\n",
    "\n",
    "display_images(all_images[:3], f\"Replaced Backgrounds with Reference Images\", resize = 256)\n",
    "display_images(all_images[3:6], resize = 256)\n",
    "display_images(all_images[6:9], resize = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Editing: Generative Fill\n",
    "\n",
    "\n",
    "Focusing on the first 3 outputs, let's fix some content issues by replacing or adding objects.\n",
    "\n",
    "We'll be using Bria's Generative-Fill Control-Net, which enables to add or modify certain areas in the image, given a mask: https://huggingface.co/briaai/BRIA-2.3-ControlNet-Generative-Fill\n",
    "\n",
    "(Also available through Bria's API: https://docs.bria.ai/image-editing/endpoints/gen-fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_canny.to('cpu')\n",
    "controlnet_depth.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from replace_bg.model.pipeline_controlnet_sd_xl import StableDiffusionXLControlNetPipeline\n",
    "from replace_bg.model.controlnet import ControlNetModel, ControlNetConditioningEmbedding\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "def resize_image_to_retain_ratio(image):\n",
    "    pixel_number = 1024*1024\n",
    "    granularity_val = 8\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    width = int((pixel_number * ratio) ** 0.5)\n",
    "    width = width - (width % granularity_val)\n",
    "    height = int(pixel_number / width)\n",
    "    height = height - (height % granularity_val)\n",
    "\n",
    "    image = image.resize((width, height))\n",
    "    return image\n",
    "\n",
    "def get_masked_image(image, image_mask, width, height):\n",
    "    image_mask = image_mask # fill area is white\n",
    "    image_mask = image_mask.resize((width, height)) # object to remove is white (1)\n",
    "    image_mask_pil = image_mask\n",
    "    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "    image_mask = np.array(image_mask_pil.convert(\"L\")).astype(np.float32) / 255.0\n",
    "    assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n",
    "    masked_image_to_present = image.copy()\n",
    "    masked_image_to_present[image_mask > 0.5] = (0.5,0.5,0.5)  # set as masked pixel\n",
    "    image[image_mask > 0.5] = 0.5  # set as masked pixel - s.t. will be grey \n",
    "    image = Image.fromarray((image * 255.0).astype(np.uint8))\n",
    "    masked_image_to_present = Image.fromarray((masked_image_to_present * 255.0).astype(np.uint8))\n",
    "    return image, image_mask_pil, masked_image_to_present\n",
    "\n",
    "\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "# Load, init model    \n",
    "controlnet = ControlNetModel().from_pretrained(\"briaai/BRIA-2.3-ControlNet-Generative-Fill\", torch_dtype=torch.float16)\n",
    "\n",
    "gen_fill_pipe = StableDiffusionXLControlNetPipeline.from_pipe(\n",
    "                    replace_bg_pipe,\n",
    "                    controlnet=controlnet,\n",
    "                )\n",
    "\n",
    "vae = gen_fill_pipe.vae\n",
    "\n",
    "gen_fill_pipe.unload_ip_adapter()\n",
    "gen_fill_pipe = gen_fill_pipe.to(device=\"cuda\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fill(input_img, mask, prompt, seed=42):\n",
    "\n",
    "    default_negative_prompt = \"blurry\"\n",
    "\n",
    "    init_image = input_img.resize((1024, 1024))\n",
    "    mask_image = mask.resize((1024, 1024))\n",
    "\n",
    "\n",
    "    init_image = resize_image_to_retain_ratio(init_image)\n",
    "    width, height = init_image.size\n",
    "\n",
    "    mask_image = mask_image.convert(\"L\").resize(init_image.size)\n",
    "\n",
    "    width, height = init_image.size\n",
    "\n",
    "\n",
    "    masked_image, image_mask, masked_image_to_present = get_masked_image(init_image, mask_image, width, height)\n",
    "\n",
    "    masked_image_tensor = image_transforms(masked_image)\n",
    "    masked_image_tensor = (masked_image_tensor - 0.5) / 0.5\n",
    "\n",
    "\n",
    "    masked_image_tensor = masked_image_tensor.unsqueeze(0).to(device=\"cuda\")\n",
    "    control_latents = vae.encode(  \n",
    "            masked_image_tensor[:, :3, :, :].to(vae.dtype)\n",
    "        ).latent_dist.sample()   \n",
    "    control_latents = control_latents * vae.config.scaling_factor \n",
    "\n",
    "\n",
    "    image_mask = np.array(image_mask)[:,:]\n",
    "    mask_tensor = torch.tensor(image_mask, dtype=torch.float32)[None, ...]\n",
    "    # binarize the mask\n",
    "    mask_tensor = torch.where(mask_tensor > 128.0, 255.0, 0)       \n",
    "\n",
    "    mask_tensor = mask_tensor / 255.0\n",
    "\n",
    "    mask_tensor = mask_tensor.to(device=\"cuda\")\n",
    "    mask_resized = torch.nn.functional.interpolate(mask_tensor[None, ...], size=(control_latents.shape[2], control_latents.shape[3]), mode='nearest')\n",
    "\n",
    "    masked_image = torch.cat([control_latents, mask_resized], dim=1)\n",
    "\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    \n",
    "    gen_img = gen_fill_pipe(negative_prompt=default_negative_prompt, prompt=prompt, \n",
    "            controlnet_conditioning_scale=1.0, \n",
    "            num_inference_steps=20, \n",
    "            height=height, width=width, \n",
    "            image = masked_image,\n",
    "            init_image = init_image,     \n",
    "            mask_image = mask_tensor,\n",
    "            guidance_scale = 3,\n",
    "            generator=generator).images[0]\n",
    "\n",
    "    return gen_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is a beach scene, let's replace the party hat with some beachwear. We'll use a mask around the hat we want to replace, and specifiy the new content in \"prompt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(f'./on_prem_results/bg_gen/bg_generations/0.png')\n",
    "mask = Image.open('./on_prem_results/masks/hat_mask.png')\n",
    "prompt = \"a straw hat\"\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "\n",
    "output_var1 = gen_fill(input_image, mask, prompt)\n",
    "output_var1.save(f'./on_prem_results/bg_gen/bg_generations/0_fixed.png')\n",
    "display_images([output_var1], f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace the hat in the second image to something more suitable for winter festivities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(f'./on_prem_results/bg_gen/bg_generations/1.png')\n",
    "mask = Image.open('./on_prem_results/masks/hat_mask.png')\n",
    "prompt = \"a santa hat\"\n",
    "\n",
    "output_var2 = gen_fill(input_image, mask, prompt)\n",
    "output_var2.save(f'./on_prem_results/bg_gen/bg_generations/1_fixed.png')\n",
    "display_images([output_var2], f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a christmas tree in the brand colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(f'./on_prem_results/bg_gen/bg_generations/1_fixed.png')\n",
    "mask = Image.open('./on_prem_results/masks/tree_mask.png')\n",
    "prompt = \"a lush christmas tree with purple and pink ornaments\"\n",
    "\n",
    "display_mask(mask, input_image)\n",
    "output_var2 = gen_fill(input_image, mask, prompt)\n",
    "output_var2.save(f'./on_prem_results/bg_gen/bg_generations/1_fixed.png')\n",
    "display_images([output_var2], f\"prompt: \\n{prompt}\", font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Editing: Expand\n",
    "\n",
    "We now have our 3 image varaiations ready. But what if we wanted to use them in adds with different aspect ratios? \n",
    "\n",
    "We'll use Bria's Expand Image API to expand the image to different aspect ratios:\n",
    "\n",
    "https://docs.bria.ai/image-editing/endpoints/image-expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_var3 = Image.open(f'./on_prem_results/bg_gen/bg_generations/2.png').resize((350, 350))\n",
    "image_variations = [output_var1, output_var2, output_var3]\n",
    "display_images(image_variations, resize = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel().from_pretrained(\"briaai/BRIA-2.3-ControlNet-Inpainting\", torch_dtype=torch.float16)\n",
    "\n",
    "expand_pipe = StableDiffusionXLControlNetPipeline.from_pipe(\n",
    "                    gen_fill_pipe,\n",
    "                    controlnet=controlnet,\n",
    "                )\n",
    "\n",
    "expand_pipe = expand_pipe.to(device=\"cuda\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_left(input_image, seed=1000):\n",
    "    org_width, org_height = input_image.size\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/image_expansion\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(input_image),\n",
    "        \"canvas_size\": [\n",
    "            org_width*2,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_size\": [\n",
    "            org_width,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_location\": [\n",
    "            org_width,\n",
    "            0\n",
    "        ],\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result_url\", '')\n",
    "    image_urls = [responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n",
    "\n",
    "def expand_top(input_image, seed=1000):\n",
    "    org_width, org_height = input_image.size\n",
    "    \n",
    "    base_url = \"https://engine.prod.bria-api.com/v1/image_expansion\"\n",
    "\n",
    "    payload = {\n",
    "        \"file\": pil_image_to_base64(input_image),\n",
    "        \"canvas_size\": [\n",
    "            org_width,\n",
    "            org_height*2\n",
    "        ],\n",
    "        \"original_image_size\": [\n",
    "            org_width,\n",
    "            org_height\n",
    "        ],\n",
    "        \"original_image_location\": [\n",
    "            0,\n",
    "            org_height\n",
    "        ],\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    responses = response.json().get(\"result_url\", '')\n",
    "    image_urls = [responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_images_left = []\n",
    "for image_var in image_variations:\n",
    "    expanded_images_left.append(expand_left(image_var)[0])\n",
    "\n",
    "display_images(expanded_images_left, resize = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_images_top = []\n",
    "for image_var in image_variations:\n",
    "    expanded_images_top.append(expand_top(image_var)[0])\n",
    "\n",
    "display_images(expanded_images_top, resize = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/qr_code.png\")], resize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_utils import *\n",
    "display_images([Image.open(\"./visuals/qr_code.png\")], resize=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This notebook was tested on **Amazon EC2 G6e** Instance (NVIDIA L40S Tensor Core GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code requires a Hugging-Face token with Bria permissions, as well as an AWS account with Bedrock permissions.\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"your-access-key-id\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your-secret-access-key\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"your-huggingface-token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tailored-Generation: \n",
    "\n",
    "## LoRA Fine-Tuning Bria's Text-to-Image Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is LoRA fine-tuning?\n",
    "LoRA (Low-Rank Adaptation) is a fine-tuning method that enables efficient training of text-to-image models by adjusting only a small set of low-rank matrices rather than modifying the entire model. \n",
    "\n",
    "Instead of retraining all the parameters of a large model, LoRA inserts lightweight trainable layers into the architecture and updates only these layers during training. \n",
    "\n",
    "This significantly reduces the computational cost and memory requirements while allowing the model to quickly learn new concepts. Because the core model remains unchanged, LoRA makes it easier to deploy and switch between different fine-tuned adaptations without having to maintain multiple large model copies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning Bria's Foundation Model\n",
    "For fine-tuning, whether through LoRA or full-fine-tuning, we'll use Bria's latest **4B-Adapt** model, which is designed to provide exceptional fine-tuning capabilities for commercial use.\n",
    "\n",
    "This model excels in aligning to the tuned style while preserving an remarkably high prompt alignment. \n",
    "\n",
    "Bria-4B-Adapt weights as well as training and inference instructions can be found here: https://huggingface.co/briaai/BRIA-4B-Adapat\n",
    "\n",
    "Bria also offers an API suite to train, manage and use fine-tuned models: https://docs.bria.ai/tailored-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary script files from huggingface hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "try:\n",
    "    local_dir = os.path.dirname(__file__)\n",
    "except:\n",
    "    local_dir = '.'\n",
    "    \n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='pipeline_bria.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='transformer_bria.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='bria_utils.py', local_dir=local_dir)\n",
    "hf_hub_download(repo_id=\"briaai/BRIA-4B-Adapt\", filename='train_lora.py', local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to Fine-Tune Bria-4B-Adapt on the following Bria Elephant Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# show all elephant images from /home/ubuntu/demo_gtc/briaphant\n",
    "images_dir = \"./briaphant\"\n",
    "all_images = {f: Image.open(os.path.join(images_dir, f)) for f in os.listdir(images_dir) if f.endswith('.png')}\n",
    "make_image_grid(all_images.values(), rows=2,cols=4, resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "As training data, we're going to need a folder with all images and a csv with the image file names and captions.\n",
    "\n",
    "For caption creation we can use a VLM (Vision-Language Model) by instructing it to return a description of each image in the dataset.\n",
    "We'll choose a prefix to repeat in all captions that contain a \"trigger word\", in this case we'll name the character BriaPhant and include it in the caption: \"A 3d-render of BriaPhant, a purple elephant\".\n",
    "\n",
    "We can ask the VLM to write the caption by completing the given prefix.\n",
    "\n",
    "We'll use **Amazon Bedrock** API to call Claude Sonnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "def center_crop_image(image):\n",
    "    width, height = image.size\n",
    "    if width == height:\n",
    "        return image\n",
    "    elif width > height:\n",
    "        left = (width - height) // 2\n",
    "        right = left + height\n",
    "        top = 0\n",
    "        bottom = height\n",
    "    else:\n",
    "        top = (height - width) // 2\n",
    "        bottom = top + width\n",
    "        left = 0\n",
    "        right = width\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "# Define the prompt for the model.\n",
    "def load_images_for_bedrock(images, resize_resolution=512, center_crop=True):\n",
    "    base64_images = []\n",
    "    for img in images:\n",
    "        if center_crop:\n",
    "            img = center_crop_image(img)\n",
    "        img = img.resize((resize_resolution, resize_resolution)).convert(\"RGB\")\n",
    "\n",
    "        with io.BytesIO() as img_buffer:\n",
    "            img.save(img_buffer, format=\"JPEG\")  # Save image directly to buffer\n",
    "            img_base64 = base64.b64encode(img_buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "        base64_images.append(img_base64)\n",
    "        \n",
    "    return base64_images\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "CLAUDE_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "def infer_with_bedrock(prompt, images, model_id = CLAUDE_MODEL_ID, temperature=0., top_p=0.3, top_k=5):\n",
    "\n",
    "    content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "    for image in images:\n",
    "        content.append({\"type\": \"image\", \"source\": {\"type\":\"base64\", \"media_type\":\"image/jpeg\",\"data\": image}})\n",
    "        \n",
    "    message = {\n",
    "            \"role\": \"user\",\n",
    "                \"content\": content}  \n",
    "\n",
    "\n",
    "    native_request = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"messages\": [message],\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = bedrock_client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"content\"][0][\"text\"]\n",
    "    return response_text\n",
    "\n",
    "def generate_caption_with_claude(image, prefix):\n",
    "    images = load_images_for_bedrock([image], center_crop=True) # training code does center-crop by default, so we can feed the captioner the same\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Describe the image. Use up to 30 words, starting with: \"{prefix}\"\n",
    "\n",
    "Response format:\n",
    "{prefix} [Concise description of the image in up to 30 words]\n",
    "\"\"\"\n",
    "\n",
    "    caption = infer_with_bedrock(prompt, images, temperature=0.1)\n",
    "\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions = []\n",
    "for file_name, img in all_images.items():\n",
    "    caption = generate_caption_with_claude(img, \"A 3d-render of BriaPhant, a purple elephant\")\n",
    "    captions.append({\"file_name\": file_name, \"caption\": caption})\n",
    "    display_images([img], title=caption.replace('BriaPhant,','BriaPhant,\\n'), font_size=12, resize=350)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(captions)\n",
    "df.to_csv(\"./briaphant/metadata.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Bria's Tailored-Gen training API uses **Amazon Sagemaker** to manage and run training sessions.\n",
    "\n",
    "In this case, we can run the training with a batch size of 1 on the current EC2 instance. We'll use gradient_accumulation_steps=4 to get an effective batch-size of 4.\n",
    "\n",
    "For larger datasets and for full-fine-tuning of Bria's adaptable model (4B-Adapt), we may want to increase the batch size and use multi-GPUs, so more VRAM resources would probably be required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set up the training configuration:\n",
    "\n",
    "hf_model_id = \"briaai/BRIA-4B-Adapt\"\n",
    "output_dir = \"./briaphant_training_output\" \n",
    "data_dir = images_dir\n",
    "max_training_steps = 1500\n",
    "rank = 128\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "!python train_lora.py \\\n",
    "    --pretrained_model_name_or_path {hf_model_id} \\\n",
    "    --dataset_name {data_dir} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --max_train_steps {max_training_steps} \\\n",
    "    --rank {rank} \\\n",
    "    --train_batch_size {batch_size} \\\n",
    "    --gradient_accumulation_steps {gradient_accumulation_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is finished, we can run inference on the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pipeline_bria import BriaPipeline\n",
    "\n",
    "\n",
    "tailored_pipe = BriaPipeline.from_pretrained(\"briaai/BRIA-4B-Adapt\", torch_dtype=torch.bfloat16,trust_remote_code=True)\n",
    "tailored_pipe.to(device=\"cuda\")\n",
    "\n",
    "\n",
    "tailored_pipe.load_lora_weights(output_dir, weight_name = \"pytorch_lora_weights.safetensors\")\n",
    "# tailored_pipe.load_lora_weights(\"briaai/BRIA-4B-Adapt\", subfolder=\"example_finetuned_model\", weight_name = \"bria_elephant.safetensors\")\n",
    "\n",
    "prompt = \"A 3d-render of BriaPhant, a purple elephant, playing the trumpet\"\n",
    "\n",
    "seed = 42\n",
    "lora_scale = 1.0\n",
    "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "image = tailored_pipe(prompt=prompt, height=1024, width=1024, generator=generator, joint_attention_kwargs={\"scale\": lora_scale}, num_inference_steps=30).images[0]\n",
    "display_images([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([Image.open(\"./visuals/qr_code.png\")], resize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

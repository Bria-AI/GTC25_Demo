{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"api_token\": \"BRIA_API_TOKEN\"  # Ensure BRIA_API_TOKEN is set in your environment variables\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tailored-Generation: \n",
    "\n",
    "## LoRA Fine-Tuning Bria's Text-to-Image Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is LoRA fine-tuning?\n",
    "LoRA (Low-Rank Adaptation) is a fine-tuning method that enables efficient training of text-to-image models by adjusting only a small set of low-rank matrices rather than modifying the entire model. \n",
    "\n",
    "Instead of retraining all the parameters of a large model, LoRA inserts lightweight trainable layers into the architecture and updates only these layers during training. \n",
    "\n",
    "This significantly reduces the computational cost and memory requirements while allowing the model to quickly learn new concepts. Because the core model remains unchanged, LoRA makes it easier to deploy and switch between different fine-tuned adaptations without having to maintain multiple large model copies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning Bria's Foundation Model\n",
    "For fine-tuning, whether through LoRA or full-fine-tuning, we'll use Bria's latest **4B-Adapt** model, which is designed to provide exceptional fine-tuning capabilities for commercial use.\n",
    "\n",
    "This model excels in aligning to the tuned style while preserving an remarkably high prompt alignment. \n",
    "\n",
    "\n",
    "Bria offers an API suite to train, manage and use fine-tuned models: https://docs.bria.ai/tailored-generation\n",
    "\n",
    "Bria-4B-Adapt weights can also be fine-tuned on-prem. Training and inference instructions can be found here: https://huggingface.co/briaai/BRIA-4B-Adapat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to Fine-Tune Bria-4B-Adapt on the following Bria Elephant Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# show all elephant images from /home/ubuntu/demo_gtc/briaphant\n",
    "images_dir = \"./briaphant\"\n",
    "all_images = {f: Image.open(os.path.join(images_dir, f)) for f in os.listdir(images_dir) if f.endswith('.png')}\n",
    "make_image_grid(all_images.values(), rows=2,cols=4, resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure a few things before we can run the API.\n",
    "- IP-Type: Type of concept we're training on. In this case, we're training a model on a defined character. All supported types:\n",
    "\n",
    "    - defined_character\n",
    "    - character_variants\n",
    "    - object_variants\n",
    "    - multi_object_set\n",
    "    - stylized_scene\n",
    "    - icons\n",
    "\n",
    "- IP-Name: Required only for defined_character IP type. The name of the character (1-3 words, e.g., \"Lora\", \"Captain Smith\"). \n",
    "\n",
    "- IP-Description: Required only for defined_character and object_variants IP types. A short phrase (up to 6 words) describing only the most crucial distinguishing features of your character (e.g., \"a female character with purple hair\").\n",
    "\n",
    "- IP-Medium: Illustration / Photography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project configurations\n",
    "\n",
    "project_name = \"bria_elephant\"\n",
    "ip_type = \"defined_character\"\n",
    "ip_name = \"BriaPhant\"\n",
    "ip_description = \"a purple skinned elephant\"\n",
    "ip_medium = \"photography\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bria uses Bedrock\n",
    "- Bria uses Sagemaker for training\n",
    "- Maybe show prefix generation on style images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a project (which can serve for different trainings of the same use-case) and then create a dataset under this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create project\n",
    "response = requests.post(\n",
    "    url=\"https://engine.prod.bria-api.com/v1/tailored-gen/projects\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"project_name\": project_name,\n",
    "        \"ip_name\": ip_name,\n",
    "        \"ip_description\": ip_description,\n",
    "        \"ip_medium\": ip_medium,\n",
    "        \"ip_type\": ip_type\n",
    "    }\n",
    ")\n",
    "assert response.status_code == 201\n",
    "project_id = int(response.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "response = requests.post(\n",
    "    url=\"https://engine.prod.bria-api.com/v1/tailored-gen/datasets\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"name\": project_name,\n",
    "        \"project_id\": project_id,\n",
    "    }\n",
    ")\n",
    "assert response.status_code == 201\n",
    "dataset_id = response.json()[\"id\"]\n",
    "prefix = response.json()['caption_prefix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is created, it contains a prefix defined by the project configs. \n",
    "\n",
    "This prefix will be used to generate captions for the images in the dataset and for prompting the fine-tuned model at inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' The set prefix in this dataset is: \"{prefix}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to upload images to this dataset. We'll iterate over the directory with the elephant images. \n",
    "\n",
    "This API endpoint not only uploads images to the datset, but also automatically generates captions that complete the set prefix.\n",
    "Automatic caption creation is done by calling a VLM through the Amazon Bedrock API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload images to dataset\n",
    "\n",
    "import base64\n",
    "\n",
    "for file_name in all_images:\n",
    "    path_to_file = os.path.join(images_dir, file_name)\n",
    "    file = open(path_to_file, \"rb\").read()\n",
    "    file64 = base64_string = base64.b64encode(file).decode(\"utf-8\")\n",
    "    response = requests.post(\n",
    "        url=f\"https://engine.prod.bria-api.com/v1/tailored-gen/datasets/{dataset_id}/images\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"file\": file64,\n",
    "            \"image_name\": file_name,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once image upload is complete, we need to change the dataset status to Completed (otherwise training will be blocked):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.put(\n",
    "    url=f\"https://engine.prod.bria-api.com/v1/tailored-gen/datasets/{dataset_id}\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"status\": 'Completed'\n",
    "    }\n",
    ")\n",
    "assert response.status_code == 200\n",
    "current_dataset = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few images and their generated captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_utils import display_images\n",
    "\n",
    "##### show images and captions\n",
    "for img_data in current_dataset['images'][:4]:\n",
    "    img_file = img_data['image_name']\n",
    "    img_caption = img_data['caption']\n",
    "    display_images([Image.open(os.path.join(images_dir, img_file))], title=f'{prefix}\\n{img_caption}', font_size=12, resize=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is ready, we can create a new model and start training.\n",
    "\n",
    "Bria's API uses **Amazon SageMaker** to manage and run the training sessions initiated through these calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "name = project_name\n",
    "response = requests.post(\n",
    "    url=\"https://engine.prod.bria-api.com/v1/tailored-gen/models\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"name\": name,\n",
    "        \"description\": \"gtc_demo\",\n",
    "        \"training_version\": \"max\",\n",
    "        \"training_method\": \"automatic_training\",\n",
    "    }\n",
    ")\n",
    "assert response.status_code == 201\n",
    "model_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "response = requests.post(\n",
    "    url=f\"https://engine.prod.bria-api.com/v1/tailored-gen/models/{model_id}/start_training\",\n",
    "    headers=headers\n",
    ")\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Training is complete, use the inference API with the model-id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_utils import return_images_from_urls\n",
    "\n",
    "def tailored_gen(prompt, tailored_model, num_results=3, seed=42):\n",
    "    \n",
    "    base_url = f\"https://engine.prod.bria-api.com/v1/text-to-image/tailored/{tailored_model}\"\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt, \n",
    "        \"seed\": seed, \n",
    "        \"num_results\": num_results,\n",
    "        \"sync\": True, \n",
    "    }\n",
    "\n",
    "    response = requests.post(base_url, json=payload, headers=headers)\n",
    "    print(response.text)\n",
    "    responses = response.json().get(\"result\", [{}])\n",
    "    image_urls = [x.get(\"urls\", [None])[0] for x in responses]\n",
    "\n",
    "    return return_images_from_urls(image_urls)\n",
    "\n",
    "prompt = \"holding a pink cocktail in its trunk and wearing a small pink party hat\"\n",
    "tailored_model = model_id\n",
    "elephant_image = tailored_gen(prompt, tailored_model, num_results=1)[0]\n",
    "elephant_image.save('./api_results/elephant_image.jpg')\n",
    "display_images([elephant_image], f\"prompt: \\n{prompt}\", font_size=10)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
